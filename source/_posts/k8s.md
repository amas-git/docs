#  k8s

> TODO:
>
> - https://www.spinnaker.io/
> - https://helm.sh/

```
The history of computer science is a history of the development of abstractions that hide complexity and empower you to build ever more sophisticated applications
```

> ç†æƒ³çš„å›¢é˜Ÿåœ¨6-8äººä¹‹é—´ï¼Œå¦‚æ­¤è§„æ¨¡çš„å›¢é˜Ÿæ›´å®¹æ˜“åˆ†äº«çŸ¥è¯†ï¼Œè¿…é€Ÿçš„åšå‡ºå†³ç­–ï¼Œ

## å®‰è£…

### ä½¿ç”¨minikube

```bash
$ pacman -S minikube
# æˆ–è€…ä»å®˜æ–¹å®‰è£…æœ€æ–°
$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube
$ ./minkube start
ğŸ˜„  minikube v1.6.1 on Arch 
âœ¨  Automatically selected the 'virtualbox' driver (alternates: [none])
ğŸ’¿  Downloading VM boot image ...
    > minikube-v1.6.0.iso.sha256: 65 B / 65 B [--------------] 100.00% ? p/s 0s
    > minikube-v1.6.0.iso: 150.93 MiB / 150.93 MiB  100.00% 687.51 KiB p/s 3m45
ğŸ”¥  Creating virtualbox VM (CPUs=2, Memory=2000MB, Disk=20000MB) ...
ğŸ³  Preparing Kubernetes v1.17.0 on Docker '19.03.5' ...
ğŸ’¾  Downloading kubelet v1.17.0
ğŸ’¾  Downloading kubeadm v1.17.0
ğŸšœ  Pulling images ...
ğŸš€  Launching Kubernetes ... 

# ä¹Ÿå¯ä»¥æŒ‡å®šminikubeä½¿ç”¨çš„cpuå’Œå†…å­˜
$ minikube delete; minikube start --extra-config=kubelet.authentication-token-webhook=true --cpus 4 --memory 8192

# æŸ¥çœ‹é›†ç¾¤çš„IP
$ minkube ip
192.168.99.101

# éƒ¨ç½²echo server
$ kubectl create deployment hello --image=k8s.gcr.io/echoserver:1.10
deployment.apps/hello created

# æŸ¥çœ‹deployment
$ kubectl get deployments hello -o yaml 
```

### Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2019-12-18T09:57:30Z"
  generation: 1
  labels:
    app: hello
  name: hello
  namespace: default
  resourceVersion: "80943"
  selfLink: /apis/apps/v1/namespaces/default/deployments/hello
  uid: abf0036c-b91f-4512-996c-3ec325005843
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: hello
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hello
    spec:
      containers:
      - image: k8s.gcr.io/echoserver:1.10
        imagePullPolicy: IfNotPresent
        name: echoserver
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2019-12-18T09:58:40Z"
    lastUpdateTime: "2019-12-18T09:58:40Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2019-12-18T09:57:30Z"
    lastUpdateTime: "2019-12-18T09:58:40Z"
    message: ReplicaSet "hello-76dfd64498" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
```

```zsh
# æˆ‘ä»¬æŠŠDeploymenté€šè¿‡Serviceæš´éœ²ç»™å¤–éƒ¨
$ kubectl expose deployment hello --type=NodePort --port=8080

# æŸ¥çœ‹Service
$ kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
hello        NodePort       10.96.221.248   <none>        8080:30112/TCP   11m
$ kubectl get svc hello -o yaml
$ kubectl get endpoints hello -o yaml
```

### Service 
```yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2019-12-18T10:09:53Z"
  labels:
    app: hello
  name: hello
  namespace: default
  resourceVersion: "82411"
  selfLink: /api/v1/namespaces/default/services/hello
  uid: 37bf6aee-6dbb-461b-9624-0ec88ca53153
spec:
  clusterIP: 10.96.221.248 | None
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30112
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: hello
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
```
 - ports
   	- nodePort: èŠ‚ç‚¹IP, `--service-node-port-range` flag (default: 30000-32767).
      	- port: Serviceçš„ç«¯å£
      	- targetPort: é€šè¿‡selectoré€‰å‡ºçš„Podçš„ç«¯å£

### Endpoint

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    endpoints.kubernetes.io/last-change-trigger-time: "2019-12-18T10:09:53Z"
  creationTimestamp: "2019-12-18T10:09:53Z"
  labels:
    app: hello
  name: hello
  namespace: default
  resourceVersion: "82412"
  selfLink: /api/v1/namespaces/default/endpoints/hello
  uid: 91cced1d-7202-45a2-a229-3a4f77813e79
subsets:
- addresses:
  - ip: 172.17.0.5
    nodeName: minikube
    targetRef:
      kind: Pod
      name: hello-76dfd64498-674z2
      namespace: default
      resourceVersion: "80941"
      uid: f92988e8-3095-4a4d-a6e7-764cb6be2b14
  ports:
  - port: 8080
    protocol: TCP
```

```zsh
# ç¡®ä¿Deploymentçš„Podå·²ç»å¯åŠ¨æˆåŠŸ
$ kubectl get pod 
NAME                          READY   STATUS    RESTARTS   AGE
hello-76dfd64498-674z2        1/1     Running   0          1m

# æŸ¥çœ‹Pod
$ kubectl get pod hello-76dfd64498-674z2 -o yaml
```

### Pod
```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2019-12-18T09:57:30Z"
  generateName: hello-76dfd64498-
  labels:
    app: hello
    pod-template-hash: 76dfd64498
  name: hello-76dfd64498-674z2
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: hello-76dfd64498
    uid: 2fdb6791-dca8-4bd7-9108-a2668f408887
  resourceVersion: "80941"
  selfLink: /api/v1/namespaces/default/pods/hello-76dfd64498-674z2
  uid: f92988e8-3095-4a4d-a6e7-764cb6be2b14
spec:
  containers:
  - image: k8s.gcr.io/echoserver:1.10
    imagePullPolicy: IfNotPresent
    name: echoserver
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-vc276
      readOnly: true
  dnsPolicy: ClusterFirst
  nodeSelector: # èŠ‚ç‚¹é€‰æ‹©, å¯ä»¥æŒ‰ç…§æ ‡ç­¾ç­›é€‰èŠ‚ç‚¹
    ${key}: ${value}
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
      preferredDuringSchedulingIgnoredDuringExecution:
        
  enableServiceLinks: true
  nodeName: minikube
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-vc276
    secret:
      defaultMode: 420
      secretName: default-token-vc276
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2019-12-18T09:57:30Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2019-12-18T09:58:40Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2019-12-18T09:58:40Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2019-12-18T09:57:30Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://1d98eb60c3fdfe6459a3c0ad1ea63d8ac32128c611ce5cf30936e3e1c1cc7eae
    image: k8s.gcr.io/echoserver:1.10
    imageID: docker-pullable://k8s.gcr.io/echoserver@sha256:cb5c1bddd1b5665e1867a7fa1b5fa843a47ee433bbb75d4293888b71def53229
    lastState: {}
    name: echoserver
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2019-12-18T09:58:40Z"
  hostIP: 192.168.99.101
  phase: Running
  podIP: 172.17.0.5
  podIPs:
  - ip: 172.17.0.5
  qosClass: BestEffort
  startTime: "2019-12-18T09:57:30Z"
```

```zsh
#  ä¸‡äº‹å…·å¤‡ï¼Œå¯ä»¥ä»å¤–éƒ¨è®¿é—®æœåŠ¡äº†
$ minikube service hello --url
http://192.168.99.101:30112
# æˆ–è€…ä½¿ç”¨kubectlè·å¾—serviceçš„ç«¯å£
$ kubectl get service hello --output='jsonpath="{.spec.ports[0].nodePort}"'
"30112"
$ kubectl get service hello -o yaml | grep nodePort
  - nodePort: 30112

# å¯ä»¥é€šè¿‡èŠ‚ç‚¹Portè®¿é—®è¿™ä¸ªæœåŠ¡äº†
$ curl http://192.168.99.101:30112

Hostname: hello-76dfd64498-674z2

Pod Information:
        -no pod information available-

Server values:
        server_version=nginx: 1.13.3 - lua: 10008

Request Information:
        client_address=172.17.0.1
        method=GET
        real path=/
        query=
        request_version=1.1
        request_scheme=http
        request_uri=http://192.168.99.101:8080/

Request Headers:
        accept=*/*
        host=192.168.99.101:30112
        user-agent=curl/7.67.0

Request Body:
        -no body in request-
```

###  ä½¿ç”¨minikubeçš„docker

```zsh
# æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨minkubeçš„docker runtime, è¿™æ ·å°±ä¸ç”¨æä¸€ä¸ªRegistryäº†
$ minikube docker-env
export DOCKER_TLS_VERIFY="1"
export DOCKER_HOST="tcp://192.168.99.101:2376"
export DOCKER_CERT_PATH="/home/amas/.minikube/certs"
# Run this command to configure your shell:
# eval $(minikube docker-env)
$ eval $(minikube docker-env)
```

```bash
# æˆ‘ä»¬ä¹Ÿå¯ä»¥åˆ©ç”¨docker contextå‘½ä»¤æ¥ä¿å­˜minkubeçš„dockeré“¾æ¥ä¿¡æ¯
$ docker context create minikube  \
--default-stack-orchestrator=kubernetes  \
--kubernetes config-file=/home/amas/.kube/config \
--docker 'host=tcp://192.168.99.106:2376,ca=/home/amas/.minikube/certs/ca.pem,cert=/home/amas/.minikube/certs/cert.pem,key=/home/amas/.minikube/certs/key.pem

# æŸ¥çœ‹æ‰€æœ‰çš„docker context
$ docker context ls
NAME                DESCRIPTION                               DOCKER ENDPOINT               KUBERNETES ENDPOINT                     ORCHESTRATOR
default *           Current DOCKER_HOST based configuration   unix:///var/run/docker.sock   https://192.168.99.106:8443 (default)   swarm
minikube                                                      tcp://192.168.99.106:2376     https://192.168.99.106:8443 (default)   kubernetes

$ docker context use minikube
```







minikubeåˆ›å»ºäº†ä¸€ä¸ªå«minikubeçš„kubectrl context, å½“æˆ‘ä»¬æƒ³æ“ä½œå…¶ä»–é›†ç¾¤åï¼Œéœ€è¦åˆ‡æ¢context, æƒ³è¦å†ç”¨å›minikubeå¯ä»¥

```zsh
$ kubectl config use-context minikube
# æˆ–è€…æ˜ç¡®è®¾ç½®kubectlçš„ä¸Šä¸‹æ–‡
$ kubectl get pods --context=minikube
```





### é…ç½®Minikube

minkubeé»˜è®¤ä½¿ç”¨çš„èµ„æºå¯èƒ½ä¸å¤Ÿç”¨ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è°ƒæ•´

```bash
minikube config set cpus 4
minikube config set memory 4096
minikube config view
minikube delete || true
minikube start --vm-driver ${1-"virtualbox"}
```

### å®‰è£…é›†ç¾¤

- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

>ä½ çš„ä»»åŠ¡:
>
>1. å®‰è£…å•èŠ‚ç‚¹çš„k8s (single control-plane node)
>2. å®‰è£…å¤šèŠ‚ç‚¹çš„k8sé›†ç¾¤ (high availability )
>3. é€šè¿‡: Certified k8s: https://github.com/cncf/k8s-conformance
>4. é€šè¿‡vagrant+ansibleéƒ¨ç½²åˆ°æœ¬åœ°è™šæœº
>5. é€šè¿‡ansibleéƒ¨ç½²åˆ°Pié›†ç¾¤
>6. é€šè¿‡ansibleéƒ¨ç½²åˆ°Aws
>7. é€šè¿‡ansibleéƒ¨ç½²åˆ°DigitalOcean
>8. é€šè¿‡ansibleéƒ¨ç½²åˆ°é˜¿é‡Œäº‘
>9. ä½¿ç”¨minikube
>10. ä½¿ç”¨minishift
>11. ä½¿ç”¨iosti



 å®‰è£…æ¡ä»¶:

- OS

  - Ubuntu 16.04+
  - Debian 9+
  - CentOS 7
  - Red Hat Enterprise Linux (RHEL) 7
  - Fedora 25+
  - HypriotOS v1.0.1+
  - Container Linux (tested with 1800.6.0)

- 2GB+ RAM

- 2CPUs+

- hostname/MAC/product_uuid æ¯ä¸ªèŠ‚ç‚¹å¿…é¡»å”¯ä¸€

- ä¿æŒå¯¹åº”çš„ç«¯å£æ‰“å¼€

  - Master

    - TCP inbound 6443* : k8s API server
    - TCP inbound 2379-2380: etcd server client API @usedby kube-apiserver, etcd
    - TCP inbound 10250: kublet API @usedby Self, Control plane
    - TCP inbound 10251: kube-scheduler @usedby self
    - TCP inbound 10252: kube-controller-manager @usedby selfWorker

    - TCP inbound 10250: kublet API @usedby self,Control plane
    - TCP inbound 30000-32767: NodePort Services **

- Disable Swap

- ç¡®ä¿iptablesä¸ä½¿ç”¨nftablesä½œä¸ºåç«¯

  - Deian 10

  - Ubuntu 19.04

    ```sh
    sudo update-alternatives --set iptables /usr/sbin/iptables-legacy
    sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy
    sudo update-alternatives --set arptables /usr/sbin/arptables-legacy
    sudo update-alternatives --set ebtables /usr/sbin/ebtables-legacy
    ```

  - Fedora 29

    ```sh
    update-alternatives --set iptables /usr/sbin/iptables-legacy
    ```

- å®‰è£…Runtime

  - v1.6.0+, k8sé»˜è®¤ç”¨CRI(Container Runtime Interface)
  - v1.14.0+ï¼Œ kubeadmä¼šè‡ªåŠ¨æ£€æµ‹å®¹å™¨runtime:
    - docker: /var/run/docker.sock
    - containerd: /run/containerd/containerd.sock
    - CRI-O: /var/run/crio/crio.sock
    - æ€»æ˜¯å°½é‡ä½¿ç”¨docker

- å®‰è£…kubeadm, kubelet, kubectl

  - kubeadm: ç”¨æ¥æ­å»ºk8sé›†ç¾¤
  - kubelet: è¿è¡Œåœ¨æ‰€æœ‰æœºå™¨ä¸Šï¼Œç®¡ç†é›†ç¾¤å†…éƒ¨çš„å„ç§ç»„ä»¶
  - kubectl: æ§åˆ¶é›†ç¾¤çš„å‘½ä»¤
  - æ³¨æ„: kubeadmä¸ä¼šå®‰è£…å’Œç®¡ç†kubeletå’Œkubectrl, ä½ å¿…é¡»ä¿è¯ä¸‰è€…ä¹‹é—´å°½å¯èƒ½ä½¿ç”¨ç›¸åŒçš„ç‰ˆæœ¬(è§: https://kubernetes.io/docs/setup/release/version-skew-policy/)

- é…ç½®cgroup diver, kubletä¼šç”¨åˆ°

  - å½“ä½ ä½¿ç”¨dockeræ—¶ï¼Œkubeadmä¼šè‡ªåŠ¨æ£€æµ‹cgroupé©±åŠ¨ï¼Œå¹¶ä¸”é…ç½®åˆ°/var/lib/kubelet/kubeadm-flags.envä¸­

  - å¦‚æœä½ ä½¿ç”¨CRI: åˆ™å¿…é¡»ä¿®æ”¹/etc/default/kubeletçš„cgroup-diver: 

    ```ini
    KUBELET_EXTRA_ARGS=--cgroup-driver=<value>
    ```

  > æ³¨æ„ï¼š åªæœ‰CRIä½¿ç”¨çš„cgroupé©±åŠ¨ä¸æ˜¯cgroupfsçš„æ—¶å€™ï¼Œä½ æ‰éœ€è¦è¿™ä¹ˆåš

  - é‡æ–°å¯åŠ¨kubeletä½ éœ€è¦:

    ```sh
    systemctl daemon-reload
    systemctl restart kubelet
    ```

- k8sçš„å‘å¸ƒå‘¨æœŸä¸º9ä¸ªæœˆ
	- v1.6.x: 2019å¹´9æœˆï½2020å¹´7æœˆ
	

----
## LET'S GO

### 0x00: å®‰è£…docker

```bash
# 1. å®‰è£…dockerçš„GPG key
- name: Add an apt signing key for Docker
   apt_key:
     url: https://download.docker.com/linux/ubuntu/gpg
     state: present
      
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

# 2. å°†dockerå®˜æ–¹çš„æºåŠ å…¥åˆ°aptä¸­
- name: Add apt repository for stable version
   apt_repository:
     repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable
     state: present
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

# 3. æ›´æ–°
$ sudo apt-get update
# å¦‚æœä¸æ”¾å¿ƒï¼Œå¯ä»¥ç¡®è®¤ä¸‹docker-ceç¡®å®ä½¿ç”¨å®˜æ–¹çš„æº
$ apt-cache policy docker-ce 

# 4. å¼€å§‹å®‰è£…
  - name: Install docker and its dependecies
    apt: 
      name: "{{ packages }}"
      state: present
      update_cache: yes
    vars:
      packages:
      - docker-ce 
      - docker-ce-cli 
      - containerd.io
$ sudo apt-get install -y docker-ce docker-ce-cli containerd.io

# 5. ç¡®è®¤ä¸‹dockerè¿è¡Œçš„çŠ¶æ€
$ sudo systemctl status docker

# 6. å°†å½“å‰ç”¨æˆ·åŠ å…¥åˆ°docker groupé‡Œ
$ sudo usermod -aG docker ${USER}
# éœ€è¦é‡æ–°ç™»å½•ä¸‹æ‰èƒ½è®©æƒé™ç”Ÿæ•ˆ
$ sudo su - ${USER} 

# 7. æ£€æŸ¥ä¸‹æ˜¯å¦å¯ä»¥ä½¿ç”¨docker
$ docker info
```



å…³é—­swap

```bash
$ sudo swapoff -a  
# ä¸ºäº†ä¿è¯é‡å¯çš„æ—¶å€™ä¹Ÿå¯ä»¥å…³é—­swap, éœ€è¦æ£€æŸ¥/etc/fstabé‡Œé¢æ˜¯å¦å­˜åœ¨swapåˆ†åŒºï¼Œæœ‰çš„è¯ç”¨sedå¹²æ‰
$ sudo sed -i '/ swap / s/^/#/' /etc/fstab
```



### 0x01: å®‰è£…kubeadm, kubelet, kubectl

```bash
sudo apt-get update && sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

# å¯ä»¥æŸ¥çœ‹ä¸‹ç‰ˆæœ¬æ˜¯ä¸æ˜¯åŒ¹é…
$ kubeadm version
$ kubectl verison
$ kubelet --version
```

### 0x02: åˆå§‹åŒ–control-plane node
 1. ControlPlaneNodeå°±æ˜¯ControlPlaneç»„å»ºè¿è¡Œçš„èŠ‚ç‚¹ï¼ŒåŒ…æ‹¬
    - etcd
    - API server
    
 2. ä¸ºäº†ä¾¿äºæ—¥åå‡çº§ä¸ºé«˜å¯ç”¨é›†ç¾¤ï¼Œå¯ä»¥åŠ ä¸Š
    
     - --control-plane-endpoint <ip>
     
 3. å®‰è£…Pod network add-on
     - SEE: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network
     
 >Pod Network Add-on
 >
 >è¿™ä¸ªä¸œè¥¿å¯ä»¥è®©Podä¹‹é—´å¯ä»¥äº’ç›¸è®¿é—®, ç®€å•çš„è¯ä½¿ç”¨Flannel, ç”Ÿäº§ç¯å¢ƒä¸ºäº†å®‰å…¨å¯ä½¿ç”¨Calico

 4. kubeadm init

    - --apiserver-advertise-address
    - --control-plane-endpoint

```bash
# åˆå§‹åŒ–çš„æ—¶å€™è¦ç”¨rootæƒé™ï¼Œ è¿™ä¸ªæ—¶é—´ä¼šæ¯”è¾ƒé•¿
# æ³¨æ„--pod-network-cidrçš„è®¾ç½®å’Œä½ é€‰æ‹©çš„add-onæ’ä»¶æœ‰å…³
#  - calico: --pod-network-cidr=192.168.0.0/16
#  - flannel: --pod-network-cidr=10.244.0.0/16 
$ sudo kubeadm init --apiserver-advertise-address="192.168.50.10" --apiserver-cert-extra-sans="192.168.50.10"  --node-name k8s-master --pod-network-cidr=192.168.0.0/16
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.50.10:6443 --token tl5guk.eks9pdcjvbsxxh1d \
    --discovery-token-ca-cert-hash sha256:f52e083623e59d8fdf0d9fbfbf2177d6db94ffa8ab8da42ebb7d024c8940a14d 
```

 ```bash
# æˆåŠŸä¹‹åæç¤ºæˆ‘ä»¬ç»™å½“å‰ç”¨æˆ·é…ç½®ä¸€ä¸‹ï¼Œè¿™æ ·ä¸å¿…ç”¨rootæƒé™æ“ä½œ
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

# æœ€åæ ¹æ®æç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å®‰è£…pod networking add-onäº†ï¼Œ æˆ‘ä»¬ä½¿ç”¨calico
$ kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml

# æœ€åæˆ‘ä»¬æ£€æŸ¥ä¸‹æ˜¯å¦OK
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                      READY   STATUS              RESTARTS   AGE
kube-system   calico-kube-controllers-55754f75c-hqnfg   0/1     Pending             0          38s
kube-system   calico-node-fr4j9                         0/1     PodInitializing     0          38s
kube-system   coredns-5644d7b6d9-7hwzb                  0/1     Pending             0          21m
kube-system   coredns-5644d7b6d9-96whr                  0/1     ContainerCreating   0          21m
kube-system   etcd-k8s-master                           1/1     Running             0          20m
kube-system   kube-apiserver-k8s-master                 1/1     Running             0          19m
kube-system   kube-controller-manager-k8s-master        1/1     Running             0          19m
kube-system   kube-proxy-bfx5c                          1/1     Running             0          21m
kube-system   kube-scheduler-k8s-master                 1/1     Running             0          20m

# ä¸€ä¸”OKäº†ï¼Œè¿˜éœ€è¦åšä¸€ä»¶äº‹æƒ…ï¼Œå¤„äºå®‰å…¨å’Œç¨³å®šçš„è€ƒè™‘ï¼Œé€šå¸¸ä¸ä¼šåœ¨control-planeä¸Šè¿è¡Œpod, æˆ‘ä»¬éœ€è¦æ”¹å˜ä¸€ä¸‹
# åˆ é™¤node-role.kubernetes.io/master
# https://www.linode.com/docs/kubernetes/getting-started-with-kubernetes/
$ kubectl taint nodes --all node-role.kubernetes.io/master-
$ kubectl get nodes
$ kubectl get namespaces
 ```



### Smoke Test

ä¸€åˆ‡å°±ç»ªï¼Œæˆ‘ä»¬æ¥æµ‹è¯•ä¸€ä¸‹k8sæ˜¯ä¸æ˜¯work

```zsh
$ kubectl create deployment nginx --image=nginx
$ kubectl get pods -l app=nginx

# å»ºç«‹ä¸€ä¸ªPod
$ POD_NAME=$(kubectl get pods -l app=nginx -o jsonpath="{.items[0].metadata.name}")
$ kubectl port-forward $POD_NAME 8080:80
$ curl --head http://127.0.0.1:8080
$ kubectl exec -ti $POD_NAME -- nginx -v

# æŸ¥çœ‹æ—¥å¿—
$ kubectl logs $POD_NAME

# è¿è¡Œä¸€ä¸ªservice
$ kubectl expose deployment nginx --port 80 --type NodePort
$ NODE_PORT=$(kubectl get svc nginx --output=jsonpath='{range .spec.ports[0]}{.nodePort}')
$ print $NODE_PORT
$ 31071

# æ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥åœ¨å¤–éƒ¨ç³»ç»Ÿè®¿é—®è¿™ä¸ªç«¯å£çš„httpæœåŠ¡äº†
$ curl http://192.168.50.10:31071
```



```bash
# æ¸…é™¤kubeadmæ‰€åšçš„äº‹æƒ…
$ kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
$ kubectl delete node <node name>

# é‡å¯
$ kubeadm reset

# æ’¤é”€iptablesè®¾ç½®
$ iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
# å¦‚æœæ˜¯ipvs
$ ipvsadm -C
```



å®‰è£…è¡¥å…¨è„šæœ¬:

```bash
# BASH
$ sudo apt-get install bash-completion
$ echo 'source /usr/share/bash-completion/bash_completion' >> ~/.bashrc
$ echo 'alias k=kubectl' >>~/.bashrc
$ echo 'complete -F __start_kubectl k' >>~/.bashrc
$ kubectl completion bash > /tmp/kubect
$ sudo cp /tmp/kubect /etc/bash_completion.d/
$ . ~/.bashrc
```





ç›®å‰æ”¯æŒçš„Network Add-on:

- https://kubernetes.io/docs/concepts/cluster-administration/addons/
- [ACI](https://www.github.com/noironetworks/aci-containers) provides integrated container networking and network security with Cisco ACI.
- [Calico](https://docs.projectcalico.org/latest/getting-started/kubernetes/) is a secure L3 networking and network policy provider.
- [Canal](https://github.com/tigera/canal/tree/master/k8s-install) unites Flannel and Calico, providing networking and network policy.
- [Cilium](https://github.com/cilium/cilium) is a L3 network and network policy plugin that can enforce HTTP/API/L7 policies transparently. Both routing and overlay/encapsulation mode are supported.
- [CNI-Genie](https://github.com/Huawei-PaaS/CNI-Genie) enables Kubernetes to seamlessly connect to a choice of CNI plugins, such as Calico, Canal, Flannel, Romana, or Weave.
- [Contiv](http://contiv.github.io/) provides configurable networking (native L3 using BGP, overlay using vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich policy framework. Contiv project is fully [open sourced](http://github.com/contiv). The [installer](http://github.com/contiv/install) provides both kubeadm and non-kubeadm based installation options.
- [Contrail](http://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/), based on [Tungsten Fabric](https://tungsten.io/), is an open source, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods and bare metal workloads.
- [Flannel](https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md) is an overlay network provider that can be used with Kubernetes.
- [Knitter](https://github.com/ZTE/Knitter/) is a network solution supporting multiple networking in Kubernetes.
- [Multus](https://github.com/Intel-Corp/multus-cni) is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK and VPP based workloads in Kubernetes.
- [NSX-T](https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf) Container Plug-in (NCP) provides integration between VMware NSX-T and container orchestrators such as Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and OpenShift.
- [Nuage](https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst) is an SDN platform that provides policy-based networking between Kubernetes Pods and non-Kubernetes environments with visibility and security monitoring.
- [Romana](http://romana.io/) is a Layer 3 networking solution for pod networks that also supports the [NetworkPolicy API](https://kubernetes.io/docs/concepts/services-networking/network-policies/). Kubeadm add-on installation details available [here](https://github.com/romana/romana/tree/master/containerize).
- [Weave Net](https://www.weave.works/docs/net/latest/kube-addon/) provides networking and network policy, will carry on working on both sides of a network partition, and does not require an external database.

> æ³¨æ„: networkå¿…é¡»æœ€å…ˆéƒ¨ç½²ï¼Œ ç„¶åæ‰èƒ½å®‰è£…CoreDNS
>
> é‡Œå›½å†…å¤–kubeadmåªæ”¯æŒCNI(Container Network Interface)
>
> IPV6éœ€è¦å®‰è£…:
>
> - CNI v0.60+
> - CNI bridge
> - local-ipam
>
> æ³¨æ„: kubeadmå¼ºåˆ¶ä½¿ç”¨RBAC: https://kubernetes.io/docs/reference/access-authn-authz/rbac/, ä½ è¦ç¡®ä¿network manifestæ”¯æŒRBAC

> æ³¨æ„: Podç½‘ç»œä¸èƒ½ä¸ä¸»æœºç½‘ç»œé‡åˆ, ä½ å¯ä»¥é€šè¿‡--pod-network-cidrè®¾å®šåˆé€‚çš„CIDR

```bash
$ kubectl apply -f <add-on.yaml>
```

## æ•…éšœæ’é™¤

```bash
# å¦‚æœé‡å¯æœºå™¨å‘ç°k8sæ²¡æœ‰å¯åŠ¨ï¼Œå¯ä»¥çœ‹kubeletæ—¥å¿—
$ journalctl -xeu kubelet
```

## k8sçš„ç»„æˆ

	- kubernetes
	- containerd: å®¹å™¨æ ‡å‡†è¿è¡Œç¯å¢ƒ
	- coredns: DNS server/forwarder
	- cni: Container Network Interface
	- etcd: å¯é çš„åˆ†å¸ƒå¼kvå­˜å‚¨

## ç”Ÿäº§ç¯å¢ƒé€‰æ‹©

- çº¢å¸½çš„openshiftçš„æœ€å°åŒ–éƒ¨ç½²https://github.com/MiniShift/minishift
- CoreOSçš„tectonic: https://coreos.com/tectonic/

## Kubectl 

```bash
# kubectlå¯ä»¥ç®¡ç†å¤šä¸ªé›†ç¾¤
# æŸ¥çœ‹
$ kubectl config current-context
minikube
# ä¿®æ”¹
$ kubectl config set-context my-context --namespace=mystuff
# é…ç½®æ–‡ä»¶ï¼š ~/.kube/configä¸­
$ cat ~/.kube/config
# ä½¿ç”¨
$ kubectl config use-context my-context --namespace=mystuff

# æŸ¥çœ‹
$ kubectl get ds --namespace=kube-system kube-proxy

# ä»¥minikubeä¸ºä¾‹ï¼ŒæŸ¥çœ‹èŠ‚ç‚¹å¯åŠ¨çš„å…¨éƒ¨k8sç»„ä»¶
$ kubectl get  --namespace=kube-system all 
NAME                                   READY   STATUS    RESTARTS   AGE
pod/coredns-6955765f44-2kk4k           1/1     Running   12         30d
pod/coredns-6955765f44-z5dxj           1/1     Running   12         30d
pod/etcd-minikube                      1/1     Running   12         30d
pod/kube-addon-manager-minikube        1/1     Running   12         30d
pod/kube-apiserver-minikube            1/1     Running   12         30d
pod/kube-controller-manager-minikube   1/1     Running   12         30d
pod/kube-proxy-884mj                   1/1     Running   14         30d
pod/kube-scheduler-minikube            1/1     Running   22         30d
pod/storage-provisioner                1/1     Running   22         30d

# CoreDNS
NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   30d

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
daemonset.apps/kube-proxy   1         1         1       1            1           beta.kubernetes.io/os=linux   30d

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2/2     2            2           30d

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-6955765f44   2         2         2       30d


## DEBUG
# ç™»å½•åˆ°podä¸Š
$ kubectl logs <pod-name>
$ kubectl exec -it <pod-name> -- <cmd>
$ kubectl attach -it <pod-name>
# æ³¨æ„pod:åé¢çš„æ–‡ä»¶è·¯å¾„å¿…é¡»å»æ‰'/'
$ kubectl cp xecho-67c74f4587-clfmk:etc/hostname 1.txt 
# é€šè¿‡masterå¼€ä¸€ä¸ªéš§é“é“¾æ¥åˆ°Pod
$ kubectl port-forward <pod-name>|service/<srv-name> <local-port>:<pod-port>

# Rank, å¿…é¡»å®‰è£…heapster
$ kubectl top pod
$ kubectl node pod

# åˆ›å»ºPod
$ kubectl run --restart=Never -it --image infoblox/dnstools dnstools 
```



## ç»„ä»¶ä¹‹é—´çš„å…³ç³»

```mermaid
graph TD;
	Deployment-->ReplicaSet-->Pod;
	CronJob-->Job --> Pod;
	DaemonSet --> Pod;
	StatefulSet --> Pod;
	ReplicationController --> Pod;
	Service --> Pod;
	HPA --> Pod;
	VPA --> Pod;
	PDB --> Pod;
	Pod --> APP;
	APP --> Volume;
	Volume --> ConfigMap;
	Volume --> Secret;
	Volume --> PVC;
	Volume --> DownwardAPI
	Volume --> HostPath
	Volume --> EmptyDir
```



## kubectl

```bash
# å¯ä»¥æŸ¥çœ‹èµ„æºå¯¹è±¡çš„ç¼©å†™,æ‰€å±APIGROUPç­‰
$  kubectl api-resources
NAME                              SHORTNAMES   APIGROUP                       NAMESPACED   KIND
bindings                                                                      true         Binding
componentstatuses                 cs                                          false        ComponentStatus
configmaps                        cm                                          true         ConfigMap
endpoints                         ep                                          true         Endpoints
events                            ev                                          true         Event
limitranges                       limits                                      true         LimitRange
namespaces                        ns                                          false        Namespace
nodes                             no                                          false        Node
persistentvolumeclaims            pvc                                         true         PersistentVolumeClaim
persistentvolumes                 pv                                          false        PersistentVolume
pods                              po                                          true         Pod
podtemplates                                                                  true         PodTemplate
replicationcontrollers            rc                                          true         ReplicationController
resourcequotas                    quota                                       true         ResourceQuota
secrets                                                                       true         Secret
serviceaccounts                   sa                                          true         ServiceAccount
services                          svc                                         true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io   false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io   false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io           false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io         false        APIService
controllerrevisions                            apps                           true         ControllerRevision
daemonsets                        ds           apps                           true         DaemonSet
deployments                       deploy       apps                           true         Deployment
replicasets                       rs           apps                           true         ReplicaSet
statefulsets                      sts          apps                           true         StatefulSet
tokenreviews                                   authentication.k8s.io          false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io           true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io           false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io           false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io           false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling                    true         HorizontalPodAutoscaler
cronjobs                          cj           batch                          true         CronJob
jobs                                           batch                          true         Job
certificatesigningrequests        csr          certificates.k8s.io            false        CertificateSigningRequest
leases                                         coordination.k8s.io            true         Lease
endpointslices                                 discovery.k8s.io               true         EndpointSlice
events                            ev           events.k8s.io                  true         Event
ingresses                         ing          extensions                     true         Ingress
ingresses                         ing          networking.k8s.io              true         Ingress
networkpolicies                   netpol       networking.k8s.io              true         NetworkPolicy
runtimeclasses                                 node.k8s.io                    false        RuntimeClass
poddisruptionbudgets              pdb          policy                         true         PodDisruptionBudget
podsecuritypolicies               psp          policy                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io      false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io      false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io      true         RoleBinding
roles                                          rbac.authorization.k8s.io      true         Role
priorityclasses                   pc           scheduling.k8s.io              false        PriorityClass
csidrivers                                     storage.k8s.io                 false        CSIDriver
csinodes                                       storage.k8s.io                 false        CSINode
storageclasses                    sc           storage.k8s.io                 false        StorageClass
volumeattachments                              storage.k8s.io                 false        VolumeAttachment

```





## LABELS

> æ ‡ç­¾: ä¸€ä¸ªæˆ–å¤šä¸ªç»‘å®šåœ¨k8sèµ„æºå¯¹è±¡ä¹‹ä¸Šçš„key/valueå¯¹
>
> - åˆæ³•å­—ç¬¦é›†: [a-Z0-9.-_]
> - é•¿åº¦é™åˆ¶ï¼š key256ï¼Œ value63

```sh
$ kubectl run $pod --image=$image --replicas=$n --labels="k1=v1,k2=v2,..."
$ kubectl get pods --show-labels
$ kubectl get pods -L $label-key
$ kubectl get pods --selector="k1=v1,k2=v2,..." # or -l
```



##  POD

```yaml
containers:
  - image:
    name:
    imagePullPolicy: [IfNotPresent|Always|]
    resources: resources
```

```yaml
resources:
  requests:
    cpu:
    memory:
  limits:
    cpu:
    memory:
```

```yaml
volumes:
  - name: 
    persistentVolumeClaim:
      claimName:
```

```yaml
env:
  - name:
    valueFrom:
    configMapKeyRef:
      name: ${cm_name}
      key: $pattern
```



```yaml
apiVersion: v1
kind: Pod
metadata:
	name: kuard
spec:
	containers:
		- image:
		  imagePullPolicy: [IfNotPresent|Always|]
		  imagePullSecrets: ${secret} #-----------[ è®¿é—®registeréœ€è¦çš„ç§˜å¯† ]
		  name:
		  resources:     #------------------------[ CPU | å†…å­˜]
		  	requests:
		  	  cpu:
		  	  memory:
		  	limits:      # èµ„æºä¸Šé™
		  	  cpu:
		  	  memory:
		  volumes:       # ---------------------Â·Â·[ å­˜å‚¨ ]
		  	- name:
		  	  hostPath:  # è®¿é—®ä¸»æœºæ–‡ä»¶ç³»ç»Ÿ
		  	    path:
		  	- name:      # NFSå­˜å‚¨
		  	  nfs:
		  	    server:
		  	    path:
		  ports:
		  	- containerPort:
		  	  name:
		  	  protocal:
          livenessProbe:  # ----------------------[ æ¢æ´» ]
          	httpGet:
          	  path:
              port:
            initialDelaySecods:
            timeoutSeconds:
            periodSeconds:
            failureThreshold:
          readinessProbe: # ----------------------[ å¯æœåŠ¡ ]
            httpGet:
              path:
              pot:
            initialDelaySecods:
            timeoutSeconds:     ${n}
            periodSeconds:      ${n}
            failureThreshold:   ${n}
          startupProbe:   # ----------------------[] ï¼Œ è®¾ç½®ä¹‹åï¼Œreadinesså’Œlivenessä¼šå¤±æ•ˆï¼Œç›´åˆ°starupæˆåŠŸ
          terminationMessagePath: ${file}
          terminationMessagePolicy: [Fileæ¶ç—…å¹´ä»£ï¼šéª‘å£«ã€ç˜Ÿç–«ã€ç™¾å¹´æˆ˜äº‰ä¸é‡‘é›€èŠ±ç‹æœçš„å‡‹è½|backToLogsOnError] # æ–‡ä»¶æˆ–æ˜¯
          volumeMounts:
           - mountPath:
             name:
             readOnly:
          command: []
          args: []
          env: []
          securityContext:
            allowPrivilegeEscalation: [true|false]
            capabilities:
              drop:
            privileged: [true|false]
            readOnlyRootFilesystem: [true|false]
            runAsGroup: ${group_id}
            runAsNonRoot: [true|false]
            runAsUser: ${user_id} 
    dnsPolicy: [ClusterFirst]
    enableServiceLinks: [true|false]
    priorityClassName: ${priority_class_name} # è°ƒåº¦ä¼˜å…ˆçº§ï¼Œ ä¼šå½±å“è°ƒåº¦å™¨éƒ¨ç½²çš„ä¼˜å…ˆçº§
                                              # è°ƒåº¦é˜Ÿåˆ—ä¸­æŒ‰ç…§è¿™ä¸ªæ¥æ’åº
                                              # èµ„æºä¸å¤Ÿçš„æ—¶å€™ï¼Œè°ƒåº¦å™¨ä¼šè¯•å›¾å›æ”¶ä½ä¼˜å…ˆçº§çš„Pod
    initContainers:        
    affinity:                                 # è°ƒåº¦äº²å’Œæ€§ç­–ç•¥(nodeAffnity)
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:   # é€‰æ‹©å¿…é¡»æ»¡è¶³æ¡ä»¶çš„èŠ‚ç‚¹
          nodeSelectorTerms:
          - matchExpressions:
            - key:
              operator: In
              values: []
        preferredDuringSchedulingIgnoredDuringExecution:  # æœ€ä¼˜é€‰æ‹©æ¡ä»¶
          weight: 1
            preference:
              matchExpressions:
      podAffinity:
      podAntiAffinity:

```

```bash
$ kubectl get [po]ds 
$ kubectl get [po]ds --watch # æŒç»­ç›‘æ§
$ kubectl get po -o wide
$ kubectl --namespace=xxx get $pods
$ kubectl describe pods $pod
$ kubectl delete $pod
$ kubectl delete pod -l app=sleep 
$ kubectl port-forward [$local-port]:$remote-port # remote-port: = pod|svc port 
$ kubectl logs $pod
$ kubectl exec $pod $cmd
$ kubectl exec -it $pod [sh|ash|bash|zsh]
$ kubectl cp $pod/$path $local-path
$ kubectl label pod $pod "k=v"
$ kubectl edit pod $pod
# åˆ©ç”¨selecorè·å¾—podçš„åå­—
$ kubectl get pods --selector=$key=$value --output=jsonpath={.items..metadata.name}
```


å­˜å‚¨çš„ä½¿ç”¨, æˆ‘ä»¬ä»¥æœ€ç®€å•çš„ä½¿ç”¨èŠ‚ç‚¹æœºä¸Šçš„å­˜å‚¨ä¸ºä¾‹:
```yaml
# myapp.yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: myapp
    image: nginx
    volumeMounts:
    - name: myapp-store
      mountPath: /data
  volumes:
  - name: myapp-store
    hostPath:
      path: /data/share
```

> æ³¨æ„: é€šå¸¸æˆ‘ä»¬ä¸ä¼šç›´æ¥å»ºç«‹POD,   è€Œæ˜¯ç”±Deploymenté€šè¿‡å®¹å™¨æ¨¡æ¿æ¥äº§ç”ŸPOD, å¦å¤–æœ¬è´¨ä¸Šç”±RCæ¥ç®¡ç†çš„POD, æœ¬è´¨ä¸Šæœ‰é«˜å¯ç”¨çš„ä¿è¯, RCä¼šä¿è¯è¿è¡Œæœ€å°çš„å®ä¾‹æ•°é‡, æ€»ä¹‹ä¸è¦åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ‰‹åŠ¨èµ·POD

```sh
$ kubectl apply -f myapp.yaml
# æŸ¥çœ‹èµ„æºéƒ¨ç½²æƒ…å†µ
$ kubectl describe myapp
$ kubectl get pod
NAME                  READY   STATUS             RESTARTS   AGE
myapp                 1/1     Running            0          6m52s
$ kubectl exec myapp -- sh -c "echo hello > /data/xxx"
$ minikube ssh -- cat /data/share/xxx
hello

# ç»“è®º: å› ä¸ºPODä¼šåœ¨æœºå™¨ä¹‹é—´é£˜æ¥é£˜å», æ‰€ä»¥å®é™…æˆ‘ä»¬ä¹Ÿå¹¶ä¸ä¼šå¤ªç”¨åˆ°èŠ‚ç‚¹æœºå™¨ä¸Šçš„å­˜å‚¨èµ„æº
```



ä¿®æ”¹PODçš„hostsæ–‡ä»¶

```yaml
# myapp.yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  hostAliases:
  - ip: "127.0.0.1"
    hostnames:
    - "foo.local"
    - "bar.local"
  containers:
  - name: myapp
    image: nginx
```

```sh
$ kubectl apply -f myapp.yaml
$ kubectl exec myapp -- cat /etc/hosts
# Kubernetes-managed hosts file.
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
172.17.0.8      myapp

# Entries added by HostAliases.
127.0.0.1       foo.local       bar.local
```

## SECURITYCONTEXT

SCæœ‰å› ä½œç”¨åŸŸä¸åŒå¯åˆ†ä¸‰ç±»:

- å®¹å™¨çº§åˆ«å®‰å…¨
- PODçº§åˆ«å®‰å…¨(Pod Security Context)
- é›†ç¾¤çº§åˆ«PODå®‰å…¨ç­–ç•¥(Pod Security Policy)

```yaml

```

-privileged è¿è¡Œç‰¹æƒå®¹å™¨ 

defaultAddCapabilities å¯æ·»åŠ åˆ°å®¹å™¨çš„Capabilities 

requiredDropCapabilities ä¼šä»å®¹å™¨ä¸­åˆ é™¤çš„Capabilities

 volumes æ§åˆ¶å®¹å™¨å¯ä»¥ä½¿ç”¨å“ªäº›volume 

hostNetwork hostç½‘ç»œ hostPorts å…è®¸çš„hostç«¯å£åˆ—è¡¨ 

hostPID ä½¿ç”¨host PID namespace 

hostIPC ä½¿ç”¨host IPC namespace 

seLinux SELinux Context 

runAsUser user ID 

supplementalGroups å…è®¸çš„è¡¥å……ç”¨æˆ·ç»„ 

fsGroup volume FSGroup 

readOnlyRootFilesystem åªè¯»æ ¹æ–‡ä»¶ç³»ç»Ÿ

## DEPLOYMENT

> DEPLOYMENT+REPLICA SETä»£æ›¿æœ€åˆçš„RC, å®ç°åº”ç”¨ç®¡ç†
>
> ä¸»è¦èŒè´£:
>
> 1. å®šä¹‰PODå’ŒREPLICA SET
> 2. æ»šåŠ¨å‡çº§å’Œå¤±è´¥å›æ»š
> 3. ä¼¸ç¼©

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: $name
spec:
  paused: [true|false]       # æ˜¯å¦å…è®¸æš‚åœå‘å¸ƒ
  progressDeadlineSeconds: 5 # å‘å¸ƒå…è®¸æ‰§è¡Œçš„æœ€é•¿æ—¶é—´(ç§’)
  minReadySeconds: 0         # æ–°åˆ›å»ºçš„PODå˜ä¸ºReadyçŠ¶æ€æ‰€å…è®¸çš„æœ€å°ç­‰å¾…æ—¶é—´ï¼Œçœ‹åˆ°readinessåå‡çº§ä¸‹ä¸€ä¸ªPOD
  revisionHistoryLimit: 14   # ä¿å­˜çš„å‘å¸ƒå†å²æ•°é‡ï¼Œé»˜è®¤10
  rollbackTo:                # å¦‚æœå¤±è´¥å›é€€åˆ°å“ªä¸ª
      revision: $n           # å›é€€åˆ°å“ªä¸ªç‰ˆæœ¬
  strategy:
      type: [rollingUpdate|Recreate] # Recreateç®€å•ç²—æš´ï¼Œä¼šdowntime, rollingUpdateä¸ºé»˜è®¤
                                     # æ»šåŠ¨å‡çº§ä¸ä¼šå‡ºç°Downtime, é€šè¿‡æ–°åºŠæ¶ä¸€ä¸ªRSæ¥å®Œæˆæ»šåŠ¨
      maxUnavailable: [n|n%] # å‘å¸ƒè¿‡ç¨‹ä¸­å…è®¸ä¸å¯ç”¨çš„PODæ•°
      maxSurge: [n|n%]       # é»˜è®¤25%ï¼Œå‘å¸ƒè¿‡ç¨‹ä¸­å…è®¸ä½¿ç”¨çš„é¢å¤–PODæ•°, å‘ä¸Šå–è¯, æ‰€ä»¥ä¸ä¼šä¸º0, è‡³å°‘ä¸º1
  selector:
    matchLabels:
      ${label_key}: ${label_value}
    run: $name
  replicas: 1 # åˆ›å»ºRS
  template:   # PODæ¨¡æ¿
    metadata:
      creationTimestamp:
      labels:
        run: $name
    spec:
      containers:
      - name: $name
        image:
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30 # ä¼˜é›…åœæœºç­‰å¾…æ—¶é—´
```



```sh
# æ³¨æ„, runå‘½ä»¤å¯ä»¥åˆ›å»ºpodå’Œreplica set, ä½†æ˜¯æ–°ç‰ˆæœ¬ä¼šæ¨èä½¿ç”¨kubectl create depoymentæ¥ä»£æ›¿
$ kubectl run ng --image=nginx --replicas=3
# åˆ é™¤åˆšæ‰
$ kubectl delete deployments.apps ng

# å»ºç«‹ä¸€ä¸ªDeployment, --recordå‘½ä»¤å¯ä»¥åœ¨annotationç§ä¿ç•™ä¸€äº›æœ‰ç”¨çš„ä¿¡æ¯
$ kubectl create deployment ng --image=nginx # é»˜è®¤replica=1
$ kubectl describe deployments.app ng
Name:                   ng
...
Labels:                 app=ng
Annotations:            deployment.kubernetes.io/revision: 3
Selector:               app=ng
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge # æ³¨æ„è¿™ä¸ªæ˜¯æ»šåŠ¨å‡çº§ç­–ç•¥
Pod Template:
  Labels:  app=ng
  Containers:
   nginx:
    Image:        nginx:latest


# 1. æŸ¥çœ‹å»ºç«‹çš„deploymentèµ„æº
$ kubectl get deploment.app ng -o yaml
# 2. æŸ¥çœ‹å»ºç«‹çš„ReplicaSetèµ„æº, æ³¨æ„RSçš„å‘½åæ–¹å¼æ˜¯, ${name}-${pod-template-hash}
$ kubectl get rs
NAME            DESIRED   CURRENT   READY   AGE
ng-59487c8b6d   1         1         1       9m8s
# 3. æŸ¥çœ‹å»ºç«‹çš„Podèµ„æº
$ kubectl get pod
NAME                  READY   STATUS             RESTARTS   AGE
ng-59487c8b6d-fsdpq   1/1     Running            0          10m

# æ‰©å®¹, æ‰§è¡Œå®Œæ‰©å®¹ä¹‹å, Deploymentä¸­çš„spec.replicaä¼šç›¸åº”ä¿®æ”¹
# æ‰©å®¹è¿‡ç¨‹ä¸­, ä¼šåˆ›å»ºæ–°çš„RS, æ–°çš„RSä¼šé€æ­¥æ‰©å®¹, è€çš„RSä¼šç¼©å®¹
$ kubectl scale deployments $name --replicas=2

# ä½¿ç”¨yamlå®šä¹‰Deploymentèµ„æº
$ kubectl create -f $depoyment.yaml --record

# æ›´æ–°é•œåƒ
# kubectl set image deployments.apps ${.metadata.name} ${.spec.template.spec.containers[].name}
$ kubectl set image deployments.apps ng nginx=nginx:1.9.1 

# ä¿®æ”¹Depolyment, ä¼šæ‰“å¼€ç¼–è¾‘å™¨, ä¿®æ”¹ç»“æŸåŒ…ä¿å­˜åè‡ªåŠ¨æäº¤
$ kubectl edit deployments.apps ${.metadata.name}
# ç¼–è¾‘å®Œä¹‹å,k8så¼€å§‹æ‰§è¡Œrollout, ä½ å¯ä»¥æŸ¥çœ‹è¿™ä¸ªè¿›åº¦
$ kubectl rollout status deployment ${.metadata.name} --watch 
Waiting for deployment "ng" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "ng" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "ng" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "ng" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "ng" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "ng" rollout to finish: 2 old replicas are pending termination...
Waiting for deployment "ng" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "ng" rollout to finish: 1 old replicas are pending termination...
deployment "ng" successfully rolled out


# æŸ¥çœ‹å‡çº§å†å², å“ˆå“ˆ, è¿™æ—¶æ²¡æœ‰--recordedå°±å°´å°¬äº†, CHANGE-CAUSEä¼šæ˜¯<none>
$ kubectl rollout history deployment ng
deployment.apps/ng 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>
3         <none>
# æŸ¥çœ‹ä¸‹å…·ä½“æŸä¸ªç‰ˆæœ¬æ˜¯ä»€ä¹ˆæ ·å­
$  kubectl rollout history deployment ng --revision 1
$  kubectl rollout history deployment ng --revision 1 -o yaml

# ** ä¸€æ¬¡å¤±è´¥çš„å‡çº§ **
# æˆ‘ä»¬å¯ä»¥ç¼–è¾‘ä¸‹deployment, ä¿®æ”¹imageä¸ºä¸€ä¸ªä¸å­˜åœ¨çš„ç‰ˆæœ¬
$ kubectl rollout status deployment ng
error: deployment "ng" exceeded its progress deadline
$ kubectl get pod 
NAME                  READY   STATUS             RESTARTS   AGE
ng-648fc58687-f27gm   0/1     ImagePullBackOff   0          121m # ImagePullBackOff  
$ kubectl rollout undo deployment
deployment.apps/ng rolled back
$ kubectl rollout status deployment ng
deployment "ng" successfully rolled out

# ** ä¸€æ¬¡å¤±è´¥çš„å‡çº§ä¹‹pauseä¿®å¤ **
# ä¿®æ”¹imageä¸ºä¸å­˜åœ¨çš„ç‰ˆæœ¬
$ kubectl rollout pause deployment ng
deployment.apps/ng paused
$ kubectl describe deployments.apps ng
...
Conditions:
  Type           Status   Reaso
  ----           ------   ------
  Available      True     MinimumReplicasAvailable
  Progressing    Unknown  DeploymentPaused
# ä¿®æ”¹ä¸ºæ­£å¸¸çš„ç‰ˆæœ¬åç»§ç»­å‘å¸ƒ
$ kubectl rollout resume deployment ng  


$ kubectl delete deployments --all
$ kubectl delete deployments --selector="k1=v1,k2=v2"


# æ»šåŠ¨å‡çº§
$ kubectl apply -f $name-deployment.yaml # æ›´æ–°åï¼Œk8så°†è‡ªåŠ¨è§¦å‘rollout
# è§‚å¯Ÿè¿›åº¦
$ kubectl rollout status deployments $name
# æš‚åœå‘å¸ƒ
$ kubectl rollout pause deployments $name
# ç»§ç»­å‘å¸ƒ
$ kubectl rollout resume depoyments $name
# æŸ¥çœ‹å‘å¸ƒå†å²
$ kubectl rollout history deployment $name
# å›æ»šå‘å¸ƒ
$ kubectl rollout undo deployments $name
# æŸ¥çœ‹æŸä¸ªå‘å¸ƒç‰ˆæœ¬çš„è¯¦æƒ…
$ kubectl rollout history deploment $name --reversion=2
# å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬
$ kubectl rollout undo deployments $name 		

# è‡ªåŠ¨æ‰©å®¹
$ kubectl autoscale deployment $name --min=10 --max=15 --cpu-percent=80

$ kubectl replace
$ kubectl patch
$ kubectl set image
```



## STATEFULLSET

- æœ‰çŠ¶æ€æœåŠ¡, åˆ é™¤åå­˜å‚¨ä¼šä¿ç•™
- ç¨³å®šçš„DNS(é‡å¯æˆ–è¿ç§»åPODåå­—, HOSTåå­—ä¸å˜), æ‰€ä»¥STATEFULLSETæ€»æ˜¯è¦å’ŒHeadlessServiceä¸€èµ·ä½¿ç”¨
- æœ‰åºéƒ¨ç½², æœ‰åºæ”¶ç¼©

```sh
# DNSæ ¼å¼, å…¶ä¸­service_nameæ˜¯headless serviceçš„åå­—
$name-${1..N}.${service_name}.${namespace}.svc.${cluster_domain:=cluster.local}
```

```yaml
# æ³¨æ„è¿™ä¸ªHeadlessServiceè¦åœ¨StatefulSetä¹‹å‰åˆ›å»º
apiVersion: v1
kind: Service
metadata:
spec:
  ports:
  - port: 80
    name: ${name}
  clusterIP: None
#-----------------------
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name:
spec:
  serviceName: ${headless_service}
  replicas: ${1..N}
  template:
  podManagementPolicy: [*OrderedReady|Parallel] # å‰ä¸€ä¸ªPODå˜ä¸ºREADYåå†åˆ›å»ºä¸‹ä¸€ä¸ª|å¹¶è¡Œåˆ›å»º
  volumeClaimTemplates:
  - metadata:
      name: ${vc_name}
    spec:
      storageClassName:
      accessModes: [ ReadWriteOnce ]
      resources:
        requests: 
          storage: 1Gi
  
```



## SERVICE

SERVICEä¸»è¦æä¾›è´Ÿè½½å‡è¡¡å’ŒæœåŠ¡å‘ç°è¿™ä¸¤ä¸ªèŒè´£.



æ—¢ç„¶è¦æä¾›è´Ÿè½½å‡è¡¡, é‚£ä¹ˆSERIVCEè‡ªç„¶å¯ä»¥è¿›è¡Œæµé‡è½¬å‘, åŒ…æ‹¬

	- å°†æµé‡è·¯ç”±åˆ°PODç§
 - å°†æµé‡è·¯ç”±åˆ°å¤–éƒ¨IP
   	- æ²¡æœ‰SELECTORçš„SERVICE + ENDPOINT
    - HEADLESS SERVICE
      	-  ä¸ä¼šç»™Serviceåˆ†é…ClusterIP
      	- kube-proxyä¹Ÿä¸ä¼šæ­ç†è¿™ä¸ªSERVICE
      	- é€šå¸¸ä¹Ÿä¸ä¼šç»™è¿™ä¸ªSERVICEæ³¨å†Œå†…éƒ¨çš„DNS

Serviceçš„åŸŸå:

```sh
$svc.$ns.svc.cluster.local
```



	- L4



> ä¸€æ–¹é¢Serviceé€šè¿‡ClusterIPæˆ–DNSæˆ–NodePortæ¥æš´éœ²è‡ªå·±, å¦ä¸€æ–¹é¢ç”¨Endpointé“¾æ¥èƒŒåæä¾›æœåŠ¡çš„POD



### HEADLESS SERVICE

```

```



### SERVICEçš„ç±»å‹

- ClusterIPï¼šé»˜è®¤ç±»å‹ï¼Œè‡ªåŠ¨åˆ†é…ä¸€ä¸ªä»…clusterå†…éƒ¨å¯ä»¥è®¿é—®çš„è™šæ‹ŸIP
- NodePortï¼šåœ¨ClusterIPåŸºç¡€ä¸Šä¸ºServiceåœ¨æ¯å°æœºå™¨ä¸Šç»‘å®šä¸€ä¸ªç«¯å£ï¼Œè¿™æ ·å°±å¯ä»¥é€šè¿‡ <NodeIP>:NodePort æ¥è®¿é—®è¯¥æœåŠ¡
- LoadBalancerï¼šåœ¨NodePortçš„åŸºç¡€ä¸Šï¼Œå€ŸåŠ©cloud provideråˆ›å»ºä¸€ä¸ªå¤–éƒ¨çš„è´Ÿè½½å‡
  è¡¡å™¨ï¼Œå¹¶å°†è¯·æ±‚è½¬å‘åˆ° <NodeIP>:NodePort
- ExternalNameï¼šå°†æœåŠ¡é€šè¿‡DNS CNAMEè®°å½•æ–¹å¼è½¬å‘åˆ°æŒ‡å®šçš„åŸŸåï¼ˆé€š
  è¿‡ spec.externlName è®¾å®šï¼‰ã€‚éœ€è¦kube-dnsç‰ˆæœ¬åœ¨1.7ä»¥ä¸Š

## REPLICASET

rsä¸»è¦è§£å†³ä¸‰ç§é—®é¢˜

- å†—ä½™(Redundancy)
- æ‰©å±•(Scale)
- åˆ†ç‰‡(Shading)

```yaml
apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: $name
spec:
  replicas: $n
  template:
    metadata:
      labels:
    version: $n
    spec:
      containers:
        - name: $name
          image:
          ports:
            - containerPort:
```

```bash
# æ‰©å®¹
$ kubectl scale replicasets $name --replicas=4
# æ ¹æ®CPUè‡ªåŠ¨æ‰©å®¹ï¼Œ autoscaler
$ kubectl autoscale rs $name --min=2 --max=5 --cpu-percent=80
# HPA
$ kubectl get hpa
# åˆ é™¤RS(åŒ…å«POD)
$ kubectl delete rs $name
# åˆ é™¤RS,ä¿ç•™POD
$ kubectl delete rs $name --cascade=false
```

## REPLICATIONCONTROLLER

RCæ˜¯ä¹Ÿæ˜¯ä¸€ç§èµ„æºå¯¹è±¡, é€šå¸¸æˆ‘ä»¬ä¸ç›´æ¥ä½¿ç”¨, è€Œæ˜¯æœ‰Deploymentæ¥è¾¾åˆ°ç›®çš„. ä¸ä»…æ”¯æŒæ»šåŠ¨å‡çº§, è€Œä¸”æ”¯æŒå‘å¸ƒè®°å½•å›æ»šç­‰ç­‰

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  selector:
    $key: $value
    matchLabels:
      $key: $value
    matchExpressions:
      - key: $key
        operator: [In|Or|And|Not]
        values: []
```



## INGRESS

- æä¾›æš´éœ²é›†ç¾¤æœåŠ¡çš„åŠŸèƒ½(HTTP/HTTPS)
- L7

```yml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name:
spec:
  tls:
  - hosts:
    - $hostname
    secretName: $secret-name
  rules:
  - host:
    http:
      paths:
      - path:
        backend:
          serviceName:
          servicePort:
      - backend:
        serviceName:
        servicePort:
  backend:
    serviceName:
    servicePort: 
```



## DAEMONSET

> åœ¨å…¨éƒ¨æˆ–éƒ¨åˆ†èŠ‚ç‚¹ä¸Šè¿è¡Œä¸€ç»„POD, æ‰€ä»¥å¸¸è¢«ç”¨ä½œç³»ç»Ÿç›‘æ§, æ—¥å¿—æ‰‹æœº, ç³»ç»Ÿè½¯ä»¶ç­‰åœºæ™¯(fluent,logstash,prometheus node exporter, collectd, gmond,kube-proxy,kube-dns,ceph)

```yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
name: $name
  labels:
spec:
  template:
    metadata:
      labels:
    spec:
      containers:
        - name: $name
          image:
          resources:
            limits:
              memory: 200Mi
          requests:
              cpu: 100m
              memory: 200Mi
          volumeMounts:
           - name: $volume-name
             mountPath: $path
             readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
        - name: $volume-name
          hostPath:
            path: $name
```

```sh
# kube-proxyå°±æ˜¯ä¸€ä¸ªdaemonsetæ–¹å¼åœ¨è¿è¡Œ
$ kubectl get daemonsets.apps --all-namespaces
NAMESPACE     NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
kube-system   kube-proxy   1         1         1       1            1           beta.kubernetes.io/os=linux   20d

```



```sh
# å¯ä»¥å¯åŠ¨æ—¶è®¾ç½®kubeletå¯åŠ¨ä¸€äº›POD,å°†PODå®šä¹‰yamlæ”¾åˆ°åˆ¶å®šç›®å½•ç§å³å¯
$ kubelet --pod-manifest-path=/etc/kubernetes/manifests
```



## JOB

JOBè´Ÿè´£å¤„ç†çŸ­æš‚çš„ä¸€æ¬¡æ€§ä»»åŠ¡, æœ‰ä¸‰ç§ç±»å‹

- éå¹¶è¡ŒJOB, åˆ›å»ºä¸€ä¸ªPODç›´åˆ°å…¶æ‰§è¡ŒæˆåŠŸ
- å›ºå®šç»“æŸæ¬¡æ•°çš„JOB, .spec.completionsæ¥è®¾ç½®éœ€è¦è¾¾åˆ°çš„æˆåŠŸæ¬¡æ•°
- .spec.parallelismç”¨æ¥æ§åˆ¶åŒæ—¶å¯ä»¥è¿è¡Œå¤šå°‘ä¸ªPOD
- å¸¦å·¥ä½œé˜Ÿåˆ—çš„JOB

```yaml
apiVersion: batch/v1
kind: Job
metadata:
name: $job
spec:
  parallelism: ${n:=1}
  completions: ${n:=1}
  activeDeadlineSeconds: ${time} # 
  template:
    spec:
    containers:
      - name: kuard
      image: gcr.io/kuar-demo/kuard-amd64:blue
      imagePullPolicy: Always
      args:
        - $arg1
        - $arg2
restartPolicy: OnFailure
```

## JOBCONTROLLER

> JOB CONTROLLERè´Ÿè´£ç®¡ç†JOB, å¹¶ä¿è¯å…¶è¿è¡Œ,
>
> JOBä¸­PODçš„restartPolicyåªèƒ½æ˜¯OnFailureå’ŒNever



```bash
$ kubectl run -i $job --image=$image --restart=OnFailure -- $args
$ kubectl delete jobs $job
```

## CRONJOB

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: $cron-job
spec:
  schedule: "0 */5 * * *" # å‚è€ƒcron
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: $job
            image:
          restartPolicy: OnFailure
        startDeadlineSeconds: # ä»»åŠ¡å¼€å§‹çš„æˆªæ­¢æœŸé™  
        concurrencyPolicy: [Allow|Forbid|Replace]
```

```
$ 
```



## CONFIGMAP

> ConfigMap(cm)å¯ä»¥ç†è§£ä¸ºk8sçš„ä¸€ä¸ªå°å‹æ–‡ä»¶ç³»ç»Ÿ

```yaml
apiVersion: v1
data:
  msg: $cm
kind: ConfigMap
metadata:
  name: cm-hello
```

å¼•ç”¨æ–¹æ³•

```yaml
containers:
  - name:
    image:
    command:
      - "$env-arg"
    env:                   # ç¯å¢ƒå˜é‡æ–¹å¼å¼•ç”¨
      - name: $env-arg
        valueFrom:
          configMapKeyRef:
            name: $cm
            key: $key
    volumeMounts:          # æ–‡ä»¶æ–¹å¼å¼•ç”¨
      - name: $volume
        mountPath: $path
volumes:
  - name: $volume
    configMap:
      name: $cm
```



```bash
$ kubectl create configmap $cm --from-file=[dir|filename|key=filename]
$ kubectl create configmap $cm --from-literal="k1=v1" --from-literal="k2=v2"
$ kubectl edit cm $cm
```



## PRIORTYCLASS

```yaml
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
globalDefault: false
description: This is a very high priority Pod clas
```



## SECRET

æ©,ç”¨æ¥ä¿ç®¡ç§˜å¯†.

```sh
$ kubectl create secret generic my-secret --from-literal user=amas --from-literal password=123456  
secret/my-secret created
$ kubectl get secrets my-secret -o yaml
apiVersion: v1
data:
  password: MTIzNDU2
  user: YW1hcw==
kind: Secret
metadata:
...
type: Opaque
$ echo -n MTIzNDU2 | base64 --decode
123456
```



```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
  type: Opaque
data:
  password: ${base64}
  username: ${base64}
```



```yml
apiVersion: v1
kind: Secret
metadata:
  creationTimestamp:
  name:
type:
data:
  tls.crt:
  tls.key:
```

```bash
$ kubectl create secret [tls|generic|docker-registry] $secret-name --cert $cert-pem-file --key $key-pem-file
```



åˆ›å»ºè®¿é—® docker-registryçš„ç§˜å¯†

```sh
$ kubectl create secret docker-registry myregistrykey --docker-server
=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-passwo
rd=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
secret "myregistrykey" created.

# ä¹Ÿå¯ä»¥ä»é…ç½®æ–‡ä»¶åˆ›å»ºç§˜å¯†
$ kubectl create secret docker-registry myregistrykey --from-file="~/.docker/config.json"
```



é‚£æ€ä¹ˆå¼•ç”¨ç§˜å¯†å‘¢?

- volume
- ç¯å¢ƒå˜é‡



```yaml
# 1. Volumeæ–¹å¼
# POD 
spec:
  volumes:
  - name: secrets
    secret:
      secretName: $secret
  containers:
  ...
  volumeMounts:
    - name: secrets
      mountPath: "/etc/secrets"
      readOnly: true
```

```yaml
# 2. ç¯å¢ƒå˜é‡æ–¹å¼
# POD
spec:
  containers:
    - name: app
      ...
      env:
      - name: USER
        valueFrom:
          secretKeyRef:
            name: $secret
            key: $key
```

```yaml
# 3. å¼•ç”¨docker-registerçš„ç§˜å¯†
spec:
  containers:
    - name: foo
      image: $image
  imagePullSecrets:
    - name: ${docker-register-secret}
```



## RBAC

authentication:

- HTTP Basic
- x509 
- Token
- äº‘æœåŠ¡å•†æä¾›çš„

```sh
$ kubectl auth can-i create pods
```



##  ROLE

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1alpha1
metadata:
  namespace: ${namespace}
  name: ${role_name}
rules:
  - apiGroups: [""]                 # The API group "" indicates the core API Group.
    resources: ["pods"]
    verbs: ["get", "watch", "list"]
    nonResourceURLs: []
```



## ROLEBINDING

```yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1alpha1
metadata:
  name: read-pods
  namespace: default
subjects:
  - kind: ServiceAccount # May be "User", "Group" or "ServiceAccount"
    name: default
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```



## SERVICE  ACCOUNT

> SAä¸ºè¿è¡Œåœ¨PODä¸­çš„è¿›ç¨‹æä¾›èº«ä»½, å¯åŠ¨å®¹å™¨çš„æ—¶å€™ä¼šè‡ªåŠ¨æŒ‚åˆ°`/run/secrets/kubernetes.io/serviceaccount` ç›®å½•ä¸‹



- SAåªåœ¨æŸä¸ªå‘½åç©ºé—´ä¸‹æ˜¯å”¯ä¸€çš„
- TokenControrllerä¼šä¸æ–­æ£€æµ‹SA, å¹¶ä¸ºä¹‹åˆ›å»ºsecret
- æ¯ä¸ªnamespaceéƒ½ä¼šè‡ªåŠ¨å»ºç«‹ä¸€ä¸ª





```sh
$ kubectl create ns amas 
$ kubectl get sa --all-namespaces | grep amas
```



```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: $service_account
spec:
  serviceAccountName: build-robot
  automountServiceAccountToken: false
  ...
```

```bash
$ kubectl get secrets $secrets
$ kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "$secrets"}]}'


$ kubectl exec ${pod} ls /run/secrets/kubernetes.io/serviceaccount
ca.crt
namespace
token
```



å¦‚ä½•åŠ å¯†è¿™äº›ç§˜å¯†è¯·å‚è€ƒ: https://kubernetes.io/zh/docs/tasks/administer-cluster/encrypt-data/

```sh
# ç”Ÿæˆé•¿åº¦ä¸º32å­—èŠ‚çš„éšæœºå¯†é’¥
$ head -c 32 /dev/urandom | base64
dcgWs4zOC7Gh0589p1jItLsuRcEsub2tyXg8/zkfe7M=

# secretsä¿å­˜åˆ°etcdä¸­, å¦‚ä¸‹å‘½ä»¤å¯ä»¥è·å¾—
$ ETCDCTL_API=3 etcdctl get /registry/secrets/default/secret1 [...] | hexdump -C
```



## SERVICE ROLE

```yaml
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRole
metadata:
  name: ${svc_role_name}
spec:
  rules:
    - services: ["*"]
      paths: ["*/quotes"]
      dmethods: ["GET"]
```

```yaml
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name:${svc_role_binding_name}
spec:
subjects:
  - properties:
      source.principal: "*"
  roleRef:
    kind: ServiceRole
    name: ${svc_role_name}
```





## PVå’ŒPVC

å¬èµ·æ¥åƒæ˜¯å¡‘æ–™, å®é™…ä¸Šæ˜¯è§£å†³æ°¸ä¹…å­˜å‚¨é—®é¢˜, PVå’ŒPVCçš„å…³ç³»ç±»ä¼¼äºPODå’ŒNODEä¹‹é—´çš„å…³ç³», PV/NODEæä¾›èµ„æº, è€ŒPVCå’ŒPODæ¶ˆè€—èµ„æº.



## NAMESPACE

1. åˆ é™¤æŸä¸ªnamespace, è¯¥namespaceä¹‹ä¸‹çš„èµ„æºä¹Ÿä¼šè¢«åˆ é™¤
2. defaultå’Œkube-systemä¸èƒ½è¢«åˆ é™¤
3. PVä¸å±äºä»»ä½•namespace, PVCå±äºæŸä¸ªnamespace

## QoS

- Best-Effort
- Bustable
- Guaranteed

## NODE

```sh
# ç»™èŠ‚ç‚¹æ‰“æ ‡ç­¾
$ kubectl label nodes ${node-name} ${key}=${value}
```



## è°ƒåº¦é—®é¢˜

> 1. å¦‚ä½•å°†PODè°ƒåº¦åˆ°åˆ¶å®šçš„NODEä¸Š?

```sh
# å°†NODEè®¾ç½®ä¸ºç»´æŠ¤æ¨¡å¼
$ kubectl cordon ${node}
```



## æœåŠ¡å‘ç°

### DNS

```sh
# è°ƒè¯•DNS, å¯åŠ¨busyboxè¿›å…¥é›†ç¾¤å†…éƒ¨æ‰§è¡Œdigå‘½ä»¤å³å¯æŸ¥çœ‹é›†ç¾¤å†…dnsè§£æ
$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh
# dig
```



### ENV

```sh
${SVC}_PORT_${PORT}_TCP
${SVC}_PORT_${PORT}_TCP_ADDR
${SVC}_PORT_${PORT}_TCP_PROTO
${SVC}_SERVICE_HOST
${SVC}_SERVICE_PORT
```



## HELM

å¦‚ä½•è§£å†³è½¯ä»¶çš„å®‰è£…,å‡çº§,ç»´æŠ¤,å¸è½½?

```sh
$ helm search repo content
$ helm repo add bitnami https://charts.bitnami.com/bitnami
$ helm reop list

$ helm install ${name} ${package}                       # é»˜è®¤å®‰è£…æŸåŒ…
$ helm 
$ helm install ${name} ${package}  --dry-run            #  - æ¨¡æ‹Ÿå®‰è£…
$ helm install --namespace ${ns} ${name}                #  - å®‰è£…åˆ°æŸåå­—ç©ºé—´
$ helm install --namespace ${ns} ${name} --create-namespace # å¦‚æ²¡æœ‰è¯¥åå­—ç©ºé—´åˆ™åˆ›å»º
$ helm install ${name} ${package} --set ${key}=${value} #  - å®šåˆ¶å®‰è£…æŸåŒ…
$ helm install ${name} ${package} --value value.yaml    #  - å®šåˆ¶å®‰è£…æŸåŒ…
$ helm install ${name} --generate-name                  # å®‰è£…,è‡ªåŠ¨ç”Ÿæˆåå­—(é¿å…å†²çªé—®é¢˜)
$ helm install ${name} --generate-name --name-template "xxx-{{randAlpha 7|lower}}" # åå­—æ¨¡æ¿
# installæ¯”è¾ƒé‡è¦çš„å‡ ä¸ªå‚æ•°:
#   --wait: æ­£å¸¸helmå¾—çŸ¥manifestå·²ç»è¢«k8sæˆåŠŸapplyä¹‹å, å³è®¤ä¸ºæˆåŠŸ, å¹¶ä¸ç®¡pod,svcç­‰æ˜¯å¦çœŸæ­£è¿è¡Œèµ·æ¥,--waitå, helmä¸æ–­check k8sçš„åç»­çŠ¶æ€, ç›´é“æˆåŠŸ
#   --atomic:

$ helm upgrate --inatall ${name}                        # å¦‚å·²å®‰è£…åˆ™å‡çº§,å¦‚æœªå®‰è£…åˆ™å®‰è£…

$ helm get note ${name}                                 # è·å¾—Release Note
$ helm ls                                               # åˆ—å‡ºå·²å®‰è£…å†…å®¹

$ helm get value ${name}                                # æŸ¥çœ‹å®‰è£…é…ç½®
$ helm get value --all                                  # æŸ¥çœ‹æ‰€æœ‰å¯é…ç½®é¡¹
$ helm get manifest ${name}                             # æŸ¥çœ‹æ¨¡æ¿åŒ–ä¹‹åçš„å†…å®¹
$ helm template ${name}                                 # æŸ¥çœ‹å®é™…å®‰è£…çš„å†…å®¹

$ helm uninstall ${name}                                # å¸è½½
$ helm uninstall ${name} --keep-history                 # å¸è½½,ä½†ä¿ç•™å®‰è£…å†å²


$ helm list                                             # åˆ—å‡ºå·²ç»å®‰è£…çš„åŒ…
$ helm list --all-namespaces

$ helm repo uptate                                      # æ›´æ–°åŒ…
# --force
# --cleanup-on-fail

$ helm upgrade ${name} ${package} --reuse-values        # å‡çº§


# TASK:
$ helm history  ${name}                                 # å®‰è£…å†å²
$ helm rollback ${name} ${seq}


# æ‰“åŒ…
$ helm create  hello
$ helm lint    hello                                   # æ£€æŸ¥
$ helm package hello
$ helm package 
       --dependency-update (-u) 
       --destination (-d) 
       --app-version
       --version
# .helmignore ä¸æƒ³è¢«æ‰“åŒ…çš„æ–‡ä»¶å¯ä»¥åŠ å…¥å…¶ä¸­       


# ç­¾å
$ helm package --sign --key ${email} --keyring ${cert} ${chart}
$ helm veryfy ${chart.tgz} --keyring ${public_key}
$ helm install --verify --keyring ${public_key} ${chart}

# ä¸‹è½½chart
$ helm pull ${chart} --version ${version}

# åˆ é™¤repo
$ helm repo remove ${chart}

# æ›´æ–°ä¾èµ–
$ helm dependency update


# æ’ä»¶å®‰è£…
$ helm plugin install
```



### Helmå®‰è£…çš„äº”ä¸ªé˜¶æ®µ

```
1. åŠ è½½chart
2. è§£æå®‰è£…é…ç½®(--set  --value)
3. æ‰§è¡Œæ¨¡æ¿
4. Render the YAML
5. å‘é€åˆ°k8s
```

### Helm Release

### Helm Chart

```sh
$ helm create hello
Creating hello
$ tree .
â””â”€â”€ hello
    â”œâ”€â”€ charts
    â”œâ”€â”€ Chart.yaml                    # å®‰è£…åŒ…çš„å…ƒæ•°æ®
    â”œâ”€â”€ templates                     # k8sç›¸å…³èµ„æºæ¨¡æ¿
    â”‚Â Â  â”œâ”€â”€ deployment.yaml
    â”‚Â Â  â”œâ”€â”€ _helpers.tpl
    â”‚Â Â  â”œâ”€â”€ ingress.yaml
    â”‚Â Â  â”œâ”€â”€ NOTES.txt                 # helm get note æ‰€æ˜¾ç¤ºçš„å†…å®¹
    â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
    â”‚Â Â  â”œâ”€â”€ service.yaml
    â”‚Â Â  â””â”€â”€ tests
    â”‚Â Â      â””â”€â”€ test-connection.yaml
    â””â”€â”€ values.yaml                   # æ¨¡æ¿é»˜è®¤å€¼
$ helm install my-hello hello --dry-run
NAME: my-hello
LAST DEPLOYED: Wed May 26 09:21:56 2021
NAMESPACE: default
STATUS: pending-install
REVISION: 1
HOOKS:
---
# Source: hello/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-hello-test-connection"
  labels:
    helm.sh/chart: hello-0.1.0
    app.kubernetes.io/name: hello
    app.kubernetes.io/instance: my-hello
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['my-hello:80']
  restartPolicy: Never
MANIFEST:
---
# Source: hello/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-hello
  labels:
    helm.sh/chart: hello-0.1.0
    app.kubernetes.io/name: hello
    app.kubernetes.io/instance: my-hello
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: hello/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-hello
  labels:
    helm.sh/chart: hello-0.1.0
    app.kubernetes.io/name: hello
    app.kubernetes.io/instance: my-hello
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hello
    app.kubernetes.io/instance: my-hello
---
# Source: hello/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-hello
  labels:
    helm.sh/chart: hello-0.1.0
    app.kubernetes.io/name: hello
    app.kubernetes.io/instance: my-hello
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: hello
      app.kubernetes.io/instance: my-hello
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hello
        app.kubernetes.io/instance: my-hello
    spec:
      serviceAccountName: my-hello
      securityContext:
        {}
      containers:
        - name: hello
          securityContext:
            {}
          image: "nginx:1.16.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}

NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=hello,app.kubernetes.io/instance=my-hello" -o jsonpath="{.items[0].metadata.name}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:80

```

### æ¨¡æ¿

```json
# æ³¨é‡Š
{{- /* è¿™æ˜¯æ³¨é‡Š */ -}}
# Pipelines, æ¨¡æ¿å‡½æ•°åœ¨https://github.com/Masterminds/sprigè¿™ä¸ªé¡¹ç›®é‡Œ, ç‹¬ç«‹äºhelm
product: {{ .Values.product | default "rocket" | quote }}
labels:
        {{- include "anvil.selectorLabels" . | nindent 8 }}

{{- with .Values.nodeSelector }}
  nodeSelector:
    {{- toYaml . | nindent 8 }}
{{- end }}

# trim
{{ "hello" -}}
{{- "hello" }}

# æ¨¡æ¿å‡½æ•°
# Helmå†…ç½®äº†https://github.com/Masterminds/sprig

# åŠ¨æ€è·å–k8sä¿¡æ¯
{{ (lookup "apps/v1" "Deployment" "target").metadata.annotations }}
{{ (lookup "v1" "ConfigMap" "anvil" "").items }}

# åˆ†æ”¯
{{- if .Value.igress.enabled -}}
{{- else -}}
{{- end }}

# å˜é‡
{{ $var := Values.character }} # åˆ›å»ºvarå¹¶èµ‹å€¼
character: {{ $var | default 'amas' | quote }}
{{ $var = "xxx" }}             # èµ‹å€¼

# å¾ªç¯
{{- range Values.xs }}
  - {{ . | quote }}
{{- end}}

{{- range $key, $value := .Values.map }}
  - {{ $key }} : {{ $value }}
{{- end}}
```

```json
# Helmå†…ç½®
.Release.Name
.Release.Namespace
.Release.IsInstall
.Release.IsUpgrade
.Release.Service
.CHart.Name
.Chart.Version
.Chart.AppVersion
.Chart.Annotations

# Helmå†…ç½® / k8sèƒ½åŠ›
.Capabilities.APIVersion
.Capabilities.KubVersion.Version
.Capabilities.KubeVersion.Major
.Capabilities.KubeVersion.Minor
# Helmå†…ç½® 
.Template.Name
.Template.BasePath

# 
.Files.Get ${name}
.Files.GetBytes
.Files.Glob
.Files.AsConfig
.Files.AsSecrets
.Files.Lines
```

Helmå¦‚ä½•å¯»æ‰¾kubenates?

- é€šè¿‡$KUBECONFIG ç¯å¢ƒå˜é‡



å¯ä»¥è¿™ä¹ˆç»ƒä¹ æ¨¡æ¿:

```sh
$ helm create hello
$ echo 'name: {{ .Values.name}}' >> hello/template/hello.txt
$ helm template hello --set name=zhoujiabo
...
# Source: hello/templates/hello.txt
name: zhoujiabo
...
```

### Chart.yaml

```
# Semantic Version Range
${Major}.${Minor}.${Patch}
```



```yaml
apiVersion:
name:
type: library|  # å¯ä»¥æ˜¯ä¸€èˆ¬çš„chartæˆ–è€…æ˜¯åº“
description:
dependencies:
  - name:
    version:
    # ^1.2.3 : >= 1.2.3
    # ^1.2.x : >= 1.2.3
    # ^2.3   : >= 2.3 ä¸” < 3
    # ^2.x   : >= 2.0.0 ä¸” < 3
    
    # ~1.2.3 : >= 1.2.3 ä¸” < 1.3.0
    # ~1     : >= 1     ä¸” < 2
    # ~2.3   : >= 2.3   ä¸” < 2.4
    # ~1.2.x : >= 1.2.0 ä¸” < 1.3.0
    repository: ${url-to-find-chart}
    
```

### JSON Schema

ç©¶ç«Ÿä½ çš„valueæ–‡ä»¶æ€ä¹ˆæ‰ç®—æ˜¯åˆæ³•å‘¢, é‚£å°±è¦æä¸€ä¸‹shcemaäº†.

```json
{
    "$schema": "http://json-schema.org/schema#",
    "type": "object",
    "properties": {
        "image": {
            "type": "object", 
            "properties": {
                "pullPolicy": {
                    "type": "string", 
                    "enum": ["Always", "IfNotPresent"] 
                },
                "repository": {
                    "type": "string"
                },
                "tag": {
                    "type": "string"
                }
            }
        }
    }
}
```



### æ¨¡æ¿å‡½æ•°

 https://helm.sh/docs/chart_template_guide/function_list/



### Helm Hook



### Helmæµ‹è¯•



### Chart ä»“åº“

åŒ…å«index.yaml

```yaml
apiVersion: v1
entries:
  superapp:
  - apiVersion: v2
    appVersion: 1.16.0
    created: "2020-04-27T17:46:52.60919-05:00"
    description: A Helm chart for Kubernetes
    digest: cd1f8d949aeb6a7a3c6720bfe71688d4add794881b78ad9715017581f7867db4
    name: superapp
    type: application
    urls:
    - superapp-0.1.0.tgz
    version: 0.1.0
generated: "2020-04-27T17:46:52.607943-05:00"
```

```sh
$ mkdir -p charts
$ helm repo index charts
$ cat charts/index.yaml
$ helm create ${chart}
$ helm pacage ${chart} --destination charts
$ helm repo index charts
$ (cd ${chart} && python3 -m http.server --bind 127.0.0.1:8080)
$ (cd ${chart} && python -m SimpleHTTPServer:8080)
$ helm repo add ${name} http://localhost:8080 --username ${user} --password ${password}
```



````
ä½¿ç”¨githubå»ºç«‹chartä»“åº“
````



## å‚è€ƒ

- https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/13-smoke-test.md
- https://github.com/kelseyhightower/kubernetes-the-hard-way
- https://github.com/kubernetes/examples/tree/master/